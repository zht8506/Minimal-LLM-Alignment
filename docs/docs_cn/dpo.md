DPO（Direct Preference Optimization），直接偏好优化。

**论文**：https://arxiv.org/abs/2305.18290
**代码**：https://github.com/eric-mitchell/direct-preference-optimization

# 1 DPO原理
