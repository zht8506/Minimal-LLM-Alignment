
\[ [English](README.md) | ä¸­æ–‡\]

ä½¿ç”¨æœ€å°‘çš„ä»£ç è¡Œåœ¨PyTorchä¸­å®ç°ä¸»æµçš„llm å¯¹é½ç®—æ³•ï¼

ä»ç¹æ‚çš„æ¡†æ¶ä¸­è„±ç¦»ï¼Œå­¦ä¹ å¯¹é½ç®—æ³•çš„æ ¸å¿ƒã€‚

## ğŸ”¥ æœ€æ–°

- ```[2025/9]``` æ”¯æŒSFTå’ŒDPOï¼Œä»£ç ç®€æ´ã€‚

- ```[2025/8]``` **Minimal-LLM-Alignment**å¼€æº.

## ğŸ“Š å¼€å§‹

### å®‰è£…ä¾èµ–
```bash
conda create --name myenv python=3.10
pip install -r requirements.txt
```
### è®­ç»ƒ

#### SFTè®­ç»ƒ
```bash
python train.py example/qwen2.5-sft.yml
```

#### DPOè®­ç»ƒ
```bash
python train.py example/qwen2.5-dpo.yml
```
#### å‘½ä»¤è¡Œè¦†ç›–

```bash
python train.py config.yml --overrides lr=2e-5 batch_size=32
```

## ğŸ“ æ•°æ®æ„å»ºæ–¹å¼

### ä½¿ç”¨huggingfaceæ•°æ®

ä¸ºäº†å¿«é€Ÿä¸Šæ‰‹æˆ‘ä»¬çš„é¡¹ç›®ï¼Œæˆ‘ä»¬æ”¯æŒä¸‰ä¸ª huggingface æ•°æ®é›†ï¼Œå¯ä»¥ç”¨æ¥è¿›è¡ŒDPOå’ŒSFTè®­ç»ƒã€‚è¿™äº›æ•°æ®é›†åŒ…æ‹¬ï¼š ```Anthropic/hh-rlhf```([link](https://huggingface.co/datasets/Anthropic/hh-rlhf))ã€```stanfordnlp/SHP```([link](https://huggingface.co/datasets/stanfordnlp/SHP))ã€```HuggingFaceH4/stack-exchange-preferences```([link](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)).

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒè‡ªå·±æ„å»ºæ•°æ®é›†è¿›è¡Œå¯¹é½è®­ç»ƒã€‚


### 1. SFTæ•°æ®é›†æ ¼å¼

SFTæ•°æ®é›†ä½¿ç”¨å¯¹è¯æ ¼å¼ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªå¯¹è¯åºåˆ—ï¼š

```json
[
  {
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Describe a process of making crepes."
      },
      {
        "role": "assistant",
        "content": "Making crepes is an easy and delicious process! Here are step-by-step instructions..."
      }
    ]
  }
]
```

### 2. DPOæ•°æ®é›†æ ¼å¼

DPOæ•°æ®é›†åŒ…å«åå¥½å¯¹æ¯”ä¿¡æ¯ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
- `conversations`: å¯¹è¯ä¸Šä¸‹æ–‡
- `chosen`: åå¥½å›ç­”
- `rejected`: éåå¥½å›ç­”

```json
[
  {
    "conversations": [
      {
        "from": "human",
        "value": "How can I best prepare for a job interview?"
      }
    ],
    "chosen": {
      "from": "gpt",
      "value": "Preparing for a job interview requires a combination of research, practice, and self-reflection..."
    },
    "rejected": {
      "from": "gpt",
      "value": "Here are some tips to help you prepare for a job interview..."
    }
  }
]
```


## ğŸ¯ é¢†å…ˆçš„ LLM å¯¹é½æ–¹æ³•
æ³¨æ„ï¼šéƒ¨åˆ†ç®—æ³•ç¼ºä¹å®˜æ–¹å®ç°ï¼Œå› æ­¤æˆ‘é‡‡ç”¨äº†ä¼˜ç§€çš„å¼€æºç‰ˆæœ¬ã€‚

### Offline RL
| **Release** | **Method** | **Reference** | **Notes** | **Link** |
| --- | --- | --- | --- | --- |
| 2023/05 | **DPO** | Direct preference optimization: Your language model is secretly a reward model | NeurIPS 2023 | [paper](https://arxiv.org/abs/2305.18290)/[code](https://github.com/eric-mitchell/direct-preference-optimization)|
| 2023/10 | **IPO** | A General Theoretical Paradigm to Understand Learning from Human Preferences | AISTATS 2024 | [paper](https://arxiv.org/abs/2310.12036)|
| 2024/02 | **KTO** | KTO: Model Alignment as Prospect Theoretic Optimization | ICML 2024 | [paper](https://arxiv.org/abs/2402.01306)/[code](https://github.com/ContextualAI/HALOs)|
| 2024/03 | **ORPO** | Orpo: Monolithic preference optimization without reference model | EMNLP 2024 | [paper](https://arxiv.org/abs/2403.07691)/[code](https://github.com/xfactlab/orpo)|
| 2024/05 | **SimPO** | Simpo: Simple preference optimization with a reference-free reward | NeurIPS 2024 | [paper](https://arxiv.org/abs/2405.14734)/[code](https://github.com/princeton-nlp/SimPO)|

### Online RL
| **Release** | **Method** | **Reference** | **Notes** | **Link** |
| --- | --- | --- | --- | --- |
| 2017/07 | **PPO** | Proximal Policy Optimization Algorithms | Arxiv | [paper](https://arxiv.org/abs/1707.06347)/[code](https://github.com/nikhilbarhate99/PPO-PyTorch) |
| 2017/07 | **GRPO** | Deepseekmath: Pushing the limits of mathematical reasoning in open language models | Arxiv | [paper](https://arxiv.org/abs/2402.03300)/[code](https://github.com/lsdefine/simple_GRPO) |





